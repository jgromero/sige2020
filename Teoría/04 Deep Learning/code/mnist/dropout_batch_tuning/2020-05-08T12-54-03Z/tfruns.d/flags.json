{
  "hidden_units": 100,
  "hidden_activation": "relu",
  "dropout": 0.3,
  "epsilon": 0.01,
  "batch_size": 128,
  "epochs": 5
}
